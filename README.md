# Privacy-Risk-Detection-in-LLMs
This repository presents a machine learning pipeline for detecting privacy leakage risks in Large Language Model (LLM) outputs. The project utilizes a custom-labeled dataset—generated with AI to simulate six distinct real-world LLM usage scenarios—to closely mirror the privacy risks encountered during practical deployments. By applying advanced Natural Language Processing (NLP) techniques and benchmarking multiple ML classifiers, it accurately identifies potential privacy vulnerabilities in generated text. This work demonstrates an essential step for responsible AI deployment and safe integration of LLMs into products and workflows

# Project Structure
| File Name | Description |
| Final-dataset.csv | AI generated dataset of LLM outputs highlighting privacy risks |
| LLM Output Classification.ipynb | Jupyter Notebook with detailed implementation |
| README.md |	Project documentation |
